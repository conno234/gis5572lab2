{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer: portions of the below code were developed using ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this if needed\n",
    "#!pip install geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import arcpy\n",
    "from arcpy import env\n",
    "import os\n",
    "import numpy as np\n",
    "import io\n",
    "import json\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "from osgeo import gdal\n",
    "import geopandas\n",
    "import pyproj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elevation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To begin, we will acquire elevation data from the MN GeoCommons \n",
    "url = \"https://resources.gisdata.mn.gov/pub/gdrs/data/pub/us_mn_state_dnr/elev_30m_digital_elevation_model/fgdb_elev_30m_digital_elevation_model.zip\"\n",
    "\n",
    "# We use the above url to send a GET request\n",
    "response = requests.get(url, stream=True)\n",
    "\n",
    "# Then we check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Get the total file size in bytes\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    # Initialize a tqdm progress bar with the total size (this is a big download so it helps to monitor progress)\n",
    "    progress_bar = tqdm(total=total_size, unit='B', unit_scale=True)\n",
    "\n",
    "    # We can specify the directory to save the file\n",
    "    save_path = os.path.join(\"C:\\\\Users\\\\conno\\\\OneDrive\\\\Desktop\\\\GIS 5572\", \"fgdb_elev_30m_digital_elevation_model.zip\")\n",
    "\n",
    "    # And then actually do the content saving\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        for data in response.iter_content(chunk_size=1024):\n",
    "            # Write data to file\n",
    "            f.write(data)\n",
    "            # Update the progress bar with the size of the written data\n",
    "            progress_bar.update(len(data))\n",
    "    \n",
    "    # We can then close the progress bar\n",
    "    progress_bar.close()\n",
    "    print(\"Download completed successfully.\")\n",
    "else:\n",
    "    print(\"Failed to download:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# In this next cell, we begin by specifying the path to the downloaded zip file we just downloaded\n",
    "zip_file_path = r\"C:\\Users\\conno\\OneDrive\\Desktop\\GIS 5572\\fgdb_elev_30m_digital_elevation_model.zip\"\n",
    "\n",
    "# Then we specify the directory where we want to extract the contents\n",
    "extract_dir = r\"C:\\Users\\conno\\OneDrive\\Desktop\\GIS 5572\"\n",
    "\n",
    "# We open the zip file...\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # ... and extract all the contents to the specified directory\n",
    "    zip_ref.extractall(extract_dir)\n",
    "\n",
    "print(\"Extraction completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For our QA/QC with our elevation data, we will check for the presence of null data.\n",
    "raster_name = 'digital_elevation_model_30m'\n",
    "\n",
    "# We first create a raster object\n",
    "raster = arcpy.Raster(raster_name)\n",
    "\n",
    "# Then we get the minimum and maximum values of the raster dataset\n",
    "min_value = raster.minimum\n",
    "max_value = raster.maximum\n",
    "\n",
    "# Then we get the extent of the raster dataset\n",
    "raster_extent = raster.extent\n",
    "\n",
    "# We can then check is if the raster dataset contains null values\n",
    "if min_value is None or max_value is None:\n",
    "    print(\"The raster dataset contains null values.\")\n",
    "else:\n",
    "    print(\"The raster dataset does not contain null values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to prepare the data to be stored in a PostGIS Database. First, we need to resample the raster to take up less space.\n",
    "arcpy.management.Resample(\n",
    "    in_raster=\"digital_elevation_model_30m\",\n",
    "    out_raster=r\"C:\\Users\\conno\\OneDrive\\Documents\\ArcGIS\\Projects\\GIS 5572 Lab 2\\GIS 5572 Lab 2.gdb\\digital_elevation_m_Resample\",\n",
    "    #We will be using 5 square km pixels\n",
    "    cell_size=\"5000 5000\",\n",
    "    resampling_type=\"NEAREST\"\n",
    ")\n",
    "print(\"Resample complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then we need to convert the raster to points\n",
    "# Input parameters\n",
    "inRaster = \"digital_elevation_m_Resample\"\n",
    "outPoint = \"elevation_point\"\n",
    "field = \"VALUE\" #This denotes elevation\n",
    "\n",
    "# Then we run the RasterToPoint arcpy function\n",
    "arcpy.conversion.RasterToPoint(inRaster, outPoint, field)\n",
    "print(\"Raster converted to points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we save it, let's do another check to make sure the data is there\n",
    "\n",
    "point_fc_name = 'elevation_point'\n",
    "\n",
    "# We will count the number of features in the point feature class\n",
    "num_features = arcpy.GetCount_management(point_fc_name)[0]\n",
    "\n",
    "# Then we will check if the feature class does contain any features\n",
    "if int(num_features) == 0:\n",
    "    print(\"The point feature class is empty.\")\n",
    "else:\n",
    "    print(\"The point feature class contains {} features.\".format(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can begin saving the dataset.\n",
    "fc_name = 'elevation_point'\n",
    "# To do this, I will convert the feature class into a dataframe\n",
    "# I start off by creating an empty list to store the feature attributes\n",
    "data = []\n",
    "\n",
    "# Then we will iterate through the feature class and fetch attributes\n",
    "fields = [field.name for field in arcpy.ListFields(fc_name)]\n",
    "with arcpy.da.SearchCursor(fc_name, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append({fields[i]: row[i] for i in range(len(fields))})\n",
    "\n",
    "# Then we will convert the list of dictionaries to a DataFrame\n",
    "elevation_df = pd.DataFrame(data)\n",
    "\n",
    "# To check that Display the DataFrame\n",
    "elevation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#With things taken care of with the dataframe, we can use psychopg2 to save it to my PostGIS database\n",
    "db_params = {\n",
    "    'database': 'INSERT DATABASE',\n",
    "    'user': 'INSERT USERNAME',\n",
    "    'password': 'INSERT PASSWORD',\n",
    "    'host': 'INSERT HOST ADDRESS',\n",
    "    'port': 'INSERT PORT'\n",
    "}\n",
    "\n",
    "# First we create a connection to the PostgreSQL database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "\n",
    "# Then we define the name of the PostGIS table that will be created\n",
    "table_name = 'elevation_point_postgis'\n",
    "\n",
    "# Then we define the SQLAlchemy engine\n",
    "engine = create_engine('postgresql+psycopg2://' + db_params['user'] + ':' + db_params['password'] +\n",
    "                       '@' + db_params['host'] + ':' + db_params['port'] + '/' + db_params['database'])\n",
    "\n",
    "# We can then convert the elevation_df to PostGIS table\n",
    "elevation_df.to_sql(table_name, engine, if_exists='replace', index=False, schema='public')\n",
    "\n",
    "# Then we close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Landcover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our next dataset is land classification data from MN GeoCommons\n",
    "# We will use another API call in order to grab the needed zipfile\n",
    "api_url=\"https://gisdata.mn.gov/api/3/action/package_show?id=biota-landcover-nlcd-mn-2019\"\n",
    "response = requests.get(api_url, verify=False)\n",
    "# We can then convert the grabbed response into a JSON\n",
    "landuse_json=response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then need to look for the TIF file, which we will do by looking through the JSON\n",
    "resources = landuse_json['result']['resources']\n",
    "for resource in resources:\n",
    "    if resource['format'] == 'tif':\n",
    "        #Once we find the shapefile, we can save the associated URL.\n",
    "        zip_url = resource['url']\n",
    "        #Next, we can save the zip_url to our directory as a zip file.\n",
    "        zip_filename = os.path.basename(zip_url)\n",
    "\n",
    "        # We can finally download the zip file and produce some code that can will generate a text output indicating the name of the file we have downloaded.\n",
    "        response = requests.get(zip_url)\n",
    "        if response.status_code == 200:\n",
    "            with open(zip_filename, 'wb') as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"Downloaded {zip_filename}\")\n",
    "        else:\n",
    "            print(f\"Failed to download {zip_filename}. Status code: {response.status_code}\")\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we of course need to unzip the file\n",
    "zip_file_path = r\"tif_biota_landcover_nlcd_mn_2019.zip\"\n",
    "\n",
    "#Then we define the folder where we want to extract the contents\n",
    "extracted_folder = r\"C:\\Users\\conno\\OneDrive\\Desktop\\GIS 5572\"\n",
    "\n",
    "#Then we can open the ZIP file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder)\n",
    "\n",
    "print(f\"All files have been extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local variables\n",
    "arcpy.management.Resample(\n",
    "    in_raster=\"NLCD_2019_Land_Cover.tif\",\n",
    "    out_raster=r\"C:\\Users\\conno\\OneDrive\\Documents\\ArcGIS\\Projects\\GIS 5572 Lab 2\\GIS 5572 Lab 2.gdb\\landcover_Resample\",\n",
    "    cell_size=\"5000 5000\",\n",
    "    resampling_type=\"NEAREST\"\n",
    ")\n",
    "print(\"Raster has been resampled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local variables\n",
    "inRaster = \"landcover_Resample\"\n",
    "outPoint = \"landcover_point\"\n",
    "field = \"NLCD_Land\"\n",
    "\n",
    "# Run RasterToPoint\n",
    "arcpy.conversion.RasterToPoint(inRaster, outPoint, field)\n",
    "\n",
    "print(\"Raster to point conversion complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For QA/QC purposes, let's check and see how many points have actual classes\n",
    "feature_class = r'C:\\Users\\conno\\OneDrive\\Documents\\ArcGIS\\Projects\\GIS 5572 Lab 2\\GIS 5572 Lab 2.gdb\\landcover_point'\n",
    "\n",
    "# We will first create an empty list to store the point IDs of unclassified points\n",
    "unclassified_point_ids = []\n",
    "\n",
    "# Then we will count the total number of points\n",
    "total_points = int(arcpy.GetCount_management(feature_class)[0])\n",
    "\n",
    "# We can then initialize a counter for unclassified points\n",
    "unclassified_count = 0\n",
    "\n",
    "# Next, we need to loop through the points to gather Point ID and Land Class info\n",
    "with arcpy.da.SearchCursor(feature_class, ['PointID', 'NLCD_Land']) as cursor:\n",
    "    for row in cursor:\n",
    "        point_id, nlcd_land = row\n",
    "        if nlcd_land == 'Unclassified':\n",
    "            unclassified_point_ids.append(point_id)\n",
    "            unclassified_count += 1\n",
    "\n",
    "# Then we calculate the percentage of unclassified points and print it out\n",
    "percentage_unclassified = (unclassified_count / total_points) * 100\n",
    "print(f\"Percentage of points classified as 'unclassified': {percentage_unclassified:.2f}%\")\n",
    "\n",
    "# Then we save the point IDs of unclassified points to a separate dataframe\n",
    "unclassified_points_df = pd.DataFrame({'PointID': unclassified_point_ids})\n",
    "\n",
    "# We can also save the dataframe with point IDs of unclassified points to a CSV file\n",
    "unclassified_points_df.to_csv('unclassified_points.csv', index=False)\n",
    "\n",
    "# Finally, let's check the contents of this dataframe\n",
    "print(unclassified_points_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the data ready, let's save it to a PostGIS table\n",
    "# Like with elevation, we first define the feature class name\n",
    "fc_name = 'landcover_point'\n",
    "\n",
    "# We then create an empty list to store the feature attributes\n",
    "data = []\n",
    "\n",
    "# Then we iterate through the feature class and fetch attributes\n",
    "fields = [field.name for field in arcpy.ListFields(fc_name)]\n",
    "with arcpy.da.SearchCursor(fc_name, fields) as cursor:\n",
    "    for row in cursor:\n",
    "        data.append({fields[i]: row[i] for i in range(len(fields))})\n",
    "\n",
    "# Then we convert the list of dictionaries to a DataFrame\n",
    "landcover_df = pd.DataFrame(data)\n",
    "\n",
    "# Display the DataFrame\n",
    "landcover_df\n",
    "\n",
    "#Outline the database parameters\n",
    "db_params = {\n",
    "    'database': 'INSERT DATABASE',\n",
    "    'user': 'INSERT USERNAME',\n",
    "    'password': 'INSERT PASSWORD',\n",
    "    'host': 'INSERT HOST ADDRESS',\n",
    "    'port': 'INSERT PORT'\n",
    "}\n",
    "\n",
    "# Create a connection to the PostgreSQL database\n",
    "conn = psycopg2.connect(**db_params)\n",
    "\n",
    "# Define the name of the PostGIS table\n",
    "table_name = 'landcover_point_postgis'\n",
    "\n",
    "# Define the SQLAlchemy engine\n",
    "engine = create_engine('postgresql+psycopg2://' + db_params['user'] + ':' + db_params['password'] +\n",
    "                       '@' + db_params['host'] + ':' + db_params['port'] + '/' + db_params['database'])\n",
    "\n",
    "# Convert DataFrame to PostGIS table\n",
    "landcover_df.to_sql(table_name, engine, if_exists='replace', index=False, schema='public')\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential Evapotranspiration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data is dependent on date\n",
    "# First we will prompt the user for start and end dates of the data to be called\n",
    "start_date = input(\"Enter the start date (YYYY-MM-DD): \")\n",
    "end_date = input(\"Enter the end date (YYYY-MM-DD): \")\n",
    "\n",
    "# Then this code constructs the URL using the user-input dates\n",
    "url = f\"https://ndawn.ndsu.nodak.edu/table.csv?station=78&station=174&station=118&station=87&station=124&station=184&station=2&station=93&station=183&station=156&station=70&station=173&station=185&station=187&station=119&station=4&station=82&station=120&station=71&station=103&station=116&station=114&station=3&station=115&station=121&station=61&station=181&station=60&station=122&station=5&station=91&station=182&station=117&station=6&station=92&station=123&station=95&station=148&variable=ddtpetp&year=2024&ttype=daily&quick_pick=&begin_date={start_date}&end_date={end_date}\"\n",
    "\n",
    "# We get the response\n",
    "response = requests.get(url)\n",
    "\n",
    "# And we can check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print(\"Data call works!\")\n",
    "else:\n",
    "    print(\"Data call failed. Status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data looks through and filters out the needed info from the URL response\n",
    "data = response.text\n",
    "lines = data.strip().split('\\n')[3:]\n",
    "result = '\\n'.join(lines)\n",
    "csv_file = StringIO(result)\n",
    "dataframe = pd.read_csv(csv_file)\n",
    "# In the end, it provides a dataframe of the acquired station data\n",
    "dataframe = dataframe.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean out any with potential NaN values.\n",
    "# First, let's establish a new separate dataframe\n",
    "nan_values_df = dataframe[dataframe[['Latitude', 'Longitude', 'Penman PET']].isna().any(axis=1)]\n",
    "\n",
    "\n",
    "# We will ensure that the gathered latitude, longtiude values in this dataframe are numeric\n",
    "dataframe['Latitude'] = pd.to_numeric(dataframe['Latitude'], errors='coerce')\n",
    "dataframe['Longitude'] = pd.to_numeric(dataframe['Longitude'], errors='coerce')\n",
    "dataframe['Penman PET'] = pd.to_numeric(dataframe['Penman PET'], errors='coerce')\n",
    "\n",
    "# We will then go through and clean the data of any NaN values\n",
    "df = dataframe.dropna(subset=['Penman PET'])\n",
    "\n",
    "# Then we can print the Data Frame entries without NaN values\n",
    "print(\"DataFrame without NaN values:\")\n",
    "print(df)\n",
    "\n",
    "# Then we can print the separate Data Frame entries containing rows with NaN values\n",
    "print(\"\\nDataFrame with rows containing NaN values:\")\n",
    "print(nan_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will condense the viable data frame by selecting specific columns to keep\n",
    "columns_to_keep = ['Station Name', 'Latitude', 'Longitude', 'Penman PET']\n",
    "df_modified = df[columns_to_keep]\n",
    "\n",
    "# Show modified DataFrame\n",
    "print(df_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us also check each reporting station is within Minnesota\n",
    "minnesota_bbox = (-97.239209, 43.499356, -89.491739, 49.384358)\n",
    "\n",
    "# We use the latitude and longtitude columns of the data frame to filter stations outside the bounding box\n",
    "stations_outside_bbox = df_modified[\n",
    "    (df_modified['Latitude'] < minnesota_bbox[1]) |\n",
    "    (df_modified['Latitude'] > minnesota_bbox[3]) |\n",
    "    (df_modified['Longitude'] < minnesota_bbox[0]) |\n",
    "    (df_modified['Longitude'] > minnesota_bbox[2])\n",
    "]\n",
    "\n",
    "# We can print the names of stations that fall outside the bounding box here\n",
    "if not stations_outside_bbox.empty:\n",
    "    print(\"Stations outside Minnesota bounding box:\")\n",
    "    print(stations_outside_bbox['Station Name'].tolist())\n",
    "else:\n",
    "    print(\"All stations are within Minnesota bounding box.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save this data to PostGIS database, I am converting the points to a vertices dictionary\n",
    "vertices = []\n",
    "for index, row in df_modified.iterrows():\n",
    "    # I can then extract information for each station\n",
    "    name = row['Station Name']\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "    PET = row['Penman PET']\n",
    "    # And then create a entry for each station with its latitude, longitude, name, and PET value\n",
    "    vertex = (longitude, latitude, name, PET)\n",
    "    # I can then append the vertex entry to the vertices list\n",
    "    vertices.append(vertex)\n",
    "\n",
    "print(vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once I have the vertices list saved as a feature class, I can visualize on ArcGIS Pro if needed\n",
    "workspace = arcpy.env.workspace\n",
    "spatial_reference = arcpy.SpatialReference(4326)  # WGS 1984 (EPSG code: 4326) or specify your desired coordinate system\n",
    "\n",
    "fc_name = \"PET_Points\"\n",
    "\n",
    "# We create a feature class to store the points\n",
    "arcpy.management.CreateFeatureclass(workspace, fc_name, \"POINT\", spatial_reference=spatial_reference)\n",
    "\n",
    "# Then we add a field to store the name and PET values\n",
    "arcpy.management.AddField(fc_name, \"Name\", \"TEXT\")\n",
    "arcpy.management.AddField(fc_name, \"PET\", \"FLOAT\")\n",
    "\n",
    "# Then we create an array to store point geometries\n",
    "array = arcpy.Array()\n",
    "\n",
    "# Iterate over the vertices\n",
    "for latitude, longitude, name, pet in vertices:\n",
    "    # Create a point geometry\n",
    "    point = arcpy.Point(longitude, latitude)\n",
    "    array.append(point)\n",
    "    \n",
    "    # Insert the point feature into the feature class\n",
    "    with arcpy.da.InsertCursor(fc_name, [\"SHAPE@\", \"Name\", \"PET\"]) as cursor:\n",
    "        cursor.insertRow([point, name, pet])\n",
    "\n",
    "# Then we create a point feature from the array and insert it into the feature class\n",
    "with arcpy.da.InsertCursor(fc_name, [\"SHAPE@\", \"Name\", \"PET\"]) as cursor:\n",
    "    for point, name, pet in zip(array, [vertex[2] for vertex in vertices], [vertex[3] for vertex in vertices]):\n",
    "        cursor.insertRow([point, name, pet])\n",
    "\n",
    "# Finally, we add the feature class to the current ArcGIS Pro map\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "map = aprx.listMaps()[0]\n",
    "map.addDataFromPath(fc_name)\n",
    "\n",
    "print(\"Point features created and added to the map.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, much like the last two datasets, I can save the dataframe to a PostGIS table\n",
    "db_params = {\n",
    "    'database': 'INSERT DATABASE',\n",
    "    'user': 'INSERT USERNAME',\n",
    "    'password': 'INSERT PASSWORD',\n",
    "    'host': 'INSERT HOST ADDRESS',\n",
    "    'port': 'INSERT PORT'\n",
    "}\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(**db_params)\n",
    "table_name = 'pet_point_postgis'\n",
    "engine = create_engine('postgresql+psycopg2://' + db_params['user'] + ':' + db_params['password'] +\n",
    "                       '@' + db_params['host'] + ':' + db_params['port'] + '/' + db_params['database'])\n",
    "df_modified.to_sql(table_name, engine, if_exists='replace', index=False, schema='public')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our final dataset is courtesy of Mesonet.\n",
    "# The following CSV produces a CSV of all stations in the RWIS network in Minnesota\n",
    "url = \"https://mesonet.agron.iastate.edu/sites/networks.php?network=MN_RWIS&format=csv&nohtml=on\"\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "station_df = pd.read_csv(url)\n",
    "\n",
    "# Define boundaries for Minnesota\n",
    "min_x, min_y, max_x, max_y = -97.5, 43.0, -89.0, 49.5\n",
    "\n",
    "# We can check if latitude and longitude fall within Minnesota boundaries\n",
    "within_minnesota = (station_df['lon'] >= min_x) & (station_df['lon'] <= max_x) & \\\n",
    "                   (station_df['lat'] >= min_y) & (station_df['lat'] <= max_y)\n",
    "\n",
    "# If needed, we can get names of stations outside Minnesota boundaries\n",
    "outside_minnesota = station_df[~within_minnesota]['stid']\n",
    "\n",
    "# Finally, we print \"yes\" if all stations fall within Minnesota, otherwise print \"no\" and list the names of stations outside the boundaries\n",
    "if within_minnesota.all():\n",
    "    print(\"Yes, everything is within bounds\")\n",
    "else:\n",
    "    print(\"No, there are some stations out of bounds\")\n",
    "    print(\"Stations outside Minnesota boundaries:\")\n",
    "    for station_name in outside_minnesota:\n",
    "        print(station_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This code looks through the station_df and inputs the station IDs into the below URL with a user inputted data\n",
    "def fetch_data_for_date(station_id, date, lat, lon):\n",
    "    url = f\"https://mesonet.agron.iastate.edu/api/1/obhistory.json?station={station_id}&network=MN_RWIS&date={date}&full=1\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Check if data is available for the station\n",
    "    if 'data' in data:\n",
    "        # Convert data to DataFrame\n",
    "        station_data_df = pd.DataFrame(data['data'])\n",
    "        \n",
    "        # Add station ID, latitude, and longitude columns\n",
    "        station_data_df['station_id'] = station_id\n",
    "        station_data_df['lat'] = lat\n",
    "        station_data_df['lon'] = lon\n",
    "        \n",
    "        return station_data_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# We can create an empty list to store data frames\n",
    "data_frames = []\n",
    "\n",
    "# This will prompt the user to input the date\n",
    "date = input(\"Enter the date (YYYY-MM-DD): \")\n",
    "\n",
    "# Then this code will go through each station ID in the first dataset\n",
    "for index, row in station_df.iterrows():\n",
    "    station_id = row['stid']\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    \n",
    "    # Fetch data for the specified date\n",
    "    station_data_for_date = fetch_data_for_date(station_id, date, lat, lon)\n",
    "    \n",
    "    # And then append station data for the specified date to the list of data frames\n",
    "    data_frames.append(station_data_for_date)\n",
    "\n",
    "# Concatenate all data frames in the list into one dataframe\n",
    "combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To summarize the data, this code averages out the air temperature value for each station\n",
    "averaged_df = combined_df.groupby('station_id').agg({'tmpf': 'mean', 'lat': 'first', 'lon': 'first'}).reset_index()\n",
    "\n",
    "print(averaged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next code checks for outliers based on calculated z-score\n",
    "# It doesn't remove the data--simply creates a brief list of any outliers and their station names\n",
    "averaged_df['tmpf_zscore'] = (averaged_df['tmpf'] - averaged_df['tmpf'].mean()) / averaged_df['tmpf'].std()\n",
    "\n",
    "# Define threshold for outlier detection (e.g., z-score > 3 or < -3)\n",
    "outlier_threshold = 3\n",
    "\n",
    "# Identify outliers based on z-score\n",
    "outliers = averaged_df[np.abs(averaged_df['tmpf_zscore']) > outlier_threshold]\n",
    "\n",
    "# Check if the number of outliers exceeds 10% of the total number of stations\n",
    "if len(outliers) > 0.1 * len(averaged_df):\n",
    "    print(\"Number of outliers exceeds 10% of total stations. Script terminated.\")\n",
    "    # Optionally, you might want to add further actions if the condition is met, like logging or terminating the script.\n",
    "    exit()  # This terminates the script execution\n",
    "\n",
    "print(\"Outliers:\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_vertices = []\n",
    "for index, row in averaged_df.iterrows():\n",
    "    # Extracting information for each station\n",
    "    name = row['station_id']\n",
    "    latitude = row['lat']\n",
    "    longitude = row['lon']\n",
    "    air_temp = row['tmpf']\n",
    "\n",
    "    # Creating a tuple for each station with its latitude, longitude, and name\n",
    "    vertex = (longitude, latitude, name, air_temp)\n",
    "    # Appending the vertex tuple to the vertices list\n",
    "    temp_vertices.append(vertex)\n",
    "\n",
    "print(temp_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arcpy\n",
    "\n",
    "# Define the workspace and spatial reference\n",
    "workspace = arcpy.env.workspace  # Use your desired workspace\n",
    "spatial_reference = arcpy.SpatialReference(4326)  # WGS 1984 (EPSG code: 4326) or specify your desired coordinate system\n",
    "\n",
    "# Define the name of the feature class\n",
    "fc_name_temp = \"Temperature_Points\"\n",
    "\n",
    "# Create a feature class to store the temperature points\n",
    "arcpy.management.CreateFeatureclass(workspace, fc_name_temp, \"POINT\", spatial_reference=spatial_reference)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add fields to store attributes\n",
    "arcpy.management.AddField(fc_name_temp, \"Station_ID\", \"TEXT\")\n",
    "arcpy.management.AddField(fc_name_temp, \"Air_Temperature\", \"FLOAT\")\n",
    "arcpy.management.AddField(fc_name_temp, \"Time\", \"TEXT\")\n",
    "\n",
    "\n",
    "# Create an array to store point geometries\n",
    "array_temp = arcpy.Array()\n",
    "\n",
    "for lon, lat, name, air_temp in temp_vertices:\n",
    "    # Create a point geometry\n",
    "    point_temp = arcpy.Point(lon, lat)\n",
    "    array_temp.append(point_temp)\n",
    "    \n",
    "    # Insert the point feature into the feature class\n",
    "    with arcpy.da.InsertCursor(fc_name_temp, [\"SHAPE@\", \"Station_ID\", \"Air_Temperature\"]) as cursor_temp:\n",
    "        cursor_temp.insertRow([point_temp, name, air_temp])\n",
    "\n",
    "# Add the feature class to the current ArcGIS Pro map\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "map = aprx.listMaps()[0]\n",
    "map.addDataFromPath(fc_name_temp)\n",
    "\n",
    "print(\"Temperature point features created and added to the map.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally we can save the temperature data to a PostGIS database\n",
    "db_params = {\n",
    "    'database': 'INSERT DATABASE',\n",
    "    'user': 'INSERT USERNAME',\n",
    "    'password': 'INSERT PASSWORD',\n",
    "    'host': 'INSERT HOST ADDRESS',\n",
    "    'port': 'INSERT PORT'\n",
    "}\n",
    "\n",
    "\n",
    "conn = psycopg2.connect(**db_params)\n",
    "table_name = 'temp_point_postgis'\n",
    "engine = create_engine('postgresql+psycopg2://' + db_params['user'] + ':' + db_params['password'] +\n",
    "                       '@' + db_params['host'] + ':' + db_params['port'] + '/' + db_params['database'])\n",
    "averaged_df.to_sql(table_name, engine, if_exists='replace', index=False, schema='public')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "Python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
